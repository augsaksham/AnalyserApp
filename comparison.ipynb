{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=pd.read_csv('result_2.csv')\n",
    "result5=pd.read_csv('result_5.csv')\n",
    "result10=pd.read_csv('result_1000.csv')\n",
    "result5_cleaned=pd.read_csv('result5_cleaned.csv')\n",
    "result1_cleaned=pd.read_csv('result1_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.707712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.534948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true  pred      frac\n",
       "0   1.0     1  0.707712\n",
       "1   0.0     0  0.379577\n",
       "2   0.0     0  0.534948\n",
       "3   0.0     0  0.416692\n",
       "4   0.0     0  0.420347"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(df_eval):\n",
    "    cnt=0\n",
    "    scr=0\n",
    "    for i in range(df_eval.shape[0]):\n",
    "        n=i\n",
    "        if(df_eval[\"true\"].iloc[n]==1):\n",
    "            cnt+=1\n",
    "            scr+=df_eval[\"frac\"].iloc[n]\n",
    "    print(\"Average = \",scr/cnt)\n",
    "    print(\"Accuracy = \",accuracy_score(df_eval['true'], df_eval['pred']))\n",
    "    print(\"F1 score = \",f1_score(df_eval['true'], df_eval['pred'], zero_division=1))\n",
    "    print(\"Confusion metric \")\n",
    "    print(confusion_matrix(df_eval['true'], df_eval['pred']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average =  0.6145481270219931\n",
      "Accuracy =  0.7426666666666667\n",
      "F1 score =  0.44487056567593475\n",
      "Confusion metric \n",
      "[[5756  243]\n",
      " [2073  928]]\n"
     ]
    }
   ],
   "source": [
    "fun(result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average =  0.6864851313070981\n",
      "Accuracy =  0.7505555555555555\n",
      "F1 score =  0.5901040715720285\n",
      "Confusion metric \n",
      "[[5139  860]\n",
      " [1385 1616]]\n"
     ]
    }
   ],
   "source": [
    "fun(result10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average =  0.6588369196863071\n",
      "Accuracy =  0.7478888888888889\n",
      "F1 score =  0.5623915139826422\n",
      "Confusion metric \n",
      "[[5273  726]\n",
      " [1543 1458]]\n"
     ]
    }
   ],
   "source": [
    "fun(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reason</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is crashing when i play a vedio</td>\n",
       "      <td>app crashes during playback</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but i want to connect it to the tv from one de...</td>\n",
       "      <td>want compatibility with more smart televisions</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>very helpful when and home working remotley</td>\n",
       "      <td>good app for work</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this zoom so called and missed call and mobile...</td>\n",
       "      <td>receiving incorrect phone number message</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one of my favorite apps</td>\n",
       "      <td>good for spending time</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0            the app is crashing when i play a vedio   \n",
       "1  but i want to connect it to the tv from one de...   \n",
       "2        very helpful when and home working remotley   \n",
       "3  this zoom so called and missed call and mobile...   \n",
       "4                            one of my favorite apps   \n",
       "\n",
       "                                           reason  label  \n",
       "0                     app crashes during playback    1.0  \n",
       "1  want compatibility with more smart televisions    0.0  \n",
       "2                               good app for work    0.0  \n",
       "3        receiving incorrect phone number message    0.0  \n",
       "4                          good for spending time    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval=pd.read_excel(\"evaluation.xlsx\")\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval=result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_eval['frac'].values), np.array(df_eval['true'].values), test_size=0.33, random_state=42,stratify=np.array(df_eval['true'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with Different Metrices and differnt models for the best fit and threshold on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_thresh(df_eval,scr):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(df_eval['frac'].values), np.array(df_eval['true'].values), test_size=0.33, random_state=42,stratify=np.array(df_eval['true'].values))\n",
    "    resu=[-1,0]\n",
    "    y_pred_test=[]\n",
    "    for i in range(20):\n",
    "        y_pred_train=[]\n",
    "        vl=i*0.05\n",
    "        for j in range(y_train.shape[0]):\n",
    "            y_pred_train.append((1 if X_train[j] >= vl else 0))\n",
    "        f1=-10\n",
    "        if scr==1:\n",
    "            f1=f1_score(list(y_train),y_pred_train)\n",
    "        else :\n",
    "            f1=accuracy_score(list(y_train),y_pred_train)\n",
    "        if(resu[0]<=f1):\n",
    "            resu=[f1,vl]\n",
    "\n",
    "    for j in range(y_test.shape[0]):\n",
    "            y_pred_test.append((1 if X_test[j] >= resu[1] else 0))\n",
    "\n",
    "    print(\"F1 Score is = \" if scr==1 else \"Accuracy Score is = \"  ,resu[0],\" at thresh = \",resu[1])\n",
    "    print(\"Accuracy = \",accuracy_score(list(y_test), y_pred_test))\n",
    "    print(\"F1 score = \",f1_score(list(y_test), y_pred_test, zero_division=1))\n",
    "    print(\"Confusion metric \")\n",
    "    print(confusion_matrix(list(y_test), y_pred_test))\n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is =  0.6256935552710201  at thresh =  0.55\n",
      "Accuracy =  0.7063973063973064\n",
      "F1 score =  0.6211989574283231\n",
      "Confusion metric \n",
      "[[1383  597]\n",
      " [ 275  715]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is =  0.7446102819237148  at thresh =  0.7000000000000001\n",
      "Accuracy =  0.7548821548821549\n",
      "F1 score =  0.5640718562874251\n",
      "Confusion metric \n",
      "[[1771  209]\n",
      " [ 519  471]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is =  0.6537911301859801  at thresh =  0.55\n",
      "Accuracy =  0.763973063973064\n",
      "F1 score =  0.6624939817043813\n",
      "Confusion metric \n",
      "[[1581  399]\n",
      " [ 302  688]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is =  0.7704809286898839  at thresh =  0.6000000000000001\n",
      "Accuracy =  0.7848484848484848\n",
      "F1 score =  0.6395939086294417\n",
      "Confusion metric \n",
      "[[1764  216]\n",
      " [ 423  567]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is =  0.6178416609628109  at thresh =  0.6000000000000001\n",
      "Accuracy =  0.7333333333333333\n",
      "F1 score =  0.6323119777158774\n",
      "Confusion metric \n",
      "[[1497  483]\n",
      " [ 309  681]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is =  0.7487562189054726  at thresh =  0.75\n",
      "Accuracy =  0.7572390572390573\n",
      "F1 score =  0.5416401780038144\n",
      "Confusion metric \n",
      "[[1823  157]\n",
      " [ 564  426]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is =  0.7704809286898839  at thresh =  0.6000000000000001\n",
      "Accuracy =  0.7848484848484848\n",
      "F1 score =  0.6395939086294417\n",
      "Confusion metric \n",
      "[[1764  216]\n",
      " [ 423  567]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result1_cleaned,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is =  0.6537911301859801  at thresh =  0.55\n",
      "Accuracy =  0.763973063973064\n",
      "F1 score =  0.6624939817043813\n",
      "Confusion metric \n",
      "[[1581  399]\n",
      " [ 302  688]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result1_cleaned,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is =  0.6537911301859801  at thresh =  0.55\n",
      "Accuracy =  0.763973063973064\n",
      "F1 score =  0.6624939817043813\n",
      "Confusion metric \n",
      "[[1581  399]\n",
      " [ 302  688]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result5_cleaned,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is =  0.7704809286898839  at thresh =  0.6000000000000001\n",
      "Accuracy =  0.7848484848484848\n",
      "F1 score =  0.6395939086294417\n",
      "Confusion metric \n",
      "[[1764  216]\n",
      " [ 423  567]]\n"
     ]
    }
   ],
   "source": [
    "find_thresh(result5_cleaned,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result : \n",
    "Model : Pegasus Large \n",
    "Epoch : 1\n",
    "Threshold : .60\n",
    "Accuracy : 78.5\n",
    "F1 Score : .64\n",
    "Changes In Data : Data Cleaned Using Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
